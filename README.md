# Trustworthy Experiments Core

A small, opinionated toolkit for designing and analyzing online experiments in data-driven B2C businesses.

The goal of this project is to provide **practical building blocks** for trustworthy experimentation:
- power and sample size analysis for typical B2C metrics,
- sensible defaults for metric design (ARPU, CR, LTV, retention, churn),
- helpers for segment-aware experiment planning (e.g. VIP vs non-VIP customers).

This repository is part of a broader effort to develop a framework for **trustworthy experimentation** in B2C products and services: e-commerce, subscription platforms, gaming, financial products for consumers, and other digital businesses.

## What's inside

- [`src/tecore/power.py`](src/tecore/power.py) – functions for power analysis and sample size calculation for common experiment setups (binary metrics like conversion rate, etc.).
- [`src/tecore/metrics.py`](src/tecore/metrics.py) – metric helpers (ratio metrics, linearization, etc.).
- [`src/tecore/design.py`](src/tecore/design.py) – utilities for high-level experiment design (MDE, sample size, duration).
- [`src/tecore/simulate.py`](src/tecore/simulate.py) - synthetic B2C-like data generator with pre/post aggregates and heavy-tailed revenue (useful for CUPED and ratio metrics).
- [`src/tecore/variance_reduction.py`](src/tecore/variance_reduction.py) - variance reduction utilities (currently: CUPED).

Part 1 - Fundamentals:
- [`notebooks/01_power_analysis_arpu_cr.ipynb`](notebooks/01_power_analysis_arpu_cr.ipynb) - step-by-step power analysis for ARPU and conversion rate.
- [`notebooks/02_metric_design_b2c.ipynb`](notebooks/02_metric_design_b2c.ipynb) - metric design examples for B2C, including ARPU and ratio metrics.
- [`notebooks/03_experiment_design_tradeoffs.ipynb`](notebooks/03_experiment_design_tradeoffs.ipynb) - experiment design trade-offs (MDE, sample size, duration).

Part 2 - Synthetic data and benchmarks:
- [`notebooks/04_synthetic_b2c_data_generator.ipynb`](notebooks/04_synthetic_b2c_data_generator.ipynb) - generate synthetic user-level data with pre/post periods and export scenario CSVs.
- [`notebooks/05_benchmark_arpu_vs_ratio_linearization.ipynb`](notebooks/05_benchmark_arpu_vs_ratio_linearization.ipynb) - benchmark ARPU vs ratio metrics (bootstrap and linearization) across multiple scenarios; includes empirical power/type I error estimates.
- [`notebooks/06_cuped_variance_reduction.ipynb`](notebooks/06_cuped_variance_reduction.ipynb) - CUPED for ARPU and ratio metrics (via linearization); variance reduction and empirical sensitivity.
- [`notebooks/07_cuped_on_arpu_with_outliers.ipynb`](notebooks/07_cuped_on_arpu_with_outliers.ipynb) - heavy tails and outliers: raw ARPU vs winsorization/log/trim + CUPED; CI width and empirical power comparisons.

### Data outputs (generated by notebooks)

Notebook 04 generates reusable scenario datasets (CSV) under:
- data/synthetic/

Typical files:
- data/synthetic/scenario_monetization.csv
- data/synthetic/scenario_activity.csv
- data/synthetic/scenario_mixed.csv

Notebooks 05–07 can:
- load those CSVs if present, or
- fall back to generating synthetic data on the fly.

## Who is this for?

- Product analysts and data analysts working in B2C companies,
- Growth and marketing teams that run experiments but don’t have a dedicated research group,
- Anyone who wants to move from “quick A/B tests” to more reliable, statistically sound decisions.

The focus is on **clarity and practicality**, not on covering every possible edge case. The code is meant to be read, modified, and adapted to real-world pipelines.

## Quickstart

### Install (pinned version)

```bash
pip install "git+https://github.com/Niuhych/trustworthy-experiments-core.git@v0.1.0"
```

### Install with pipx (recommended for CLI)
```
pipx install "git+https://github.com/Niuhych/trustworthy-experiments-core.git@v0.1.0"
```

### Sanity check (run on included examples)

## Validate example user-level dataset:
```bash
tecore validate --input examples/example_user_level.csv --schema b2c_user_level
```

## Run base vs CUPED on a mean metric:
```bash
tecore cuped \
  --input examples/example_user_level.csv \
  --y revenue --x revenue_pre \
  --out-md out/report_mean.md --out-json out/result_mean.json
```

## Validate example ratio dataset:
```bash
tecore validate --input examples/example_ratio.csv --schema b2c_ratio
```

## Run base vs CUPED on a ratio metric via linearization:
```bash
tecore cuped-ratio \
  --input examples/example_ratio.csv \
  --num revenue --den sessions \
  --num-pre revenue_pre --den-pre sessions_pre \
  --out-md out/report_ratio.md --out-json out/result_ratio.json
```

### Run on your data

User-level mean metric:
```bash
tecore validate --input data.csv --schema b2c_user_level
tecore cuped --input data.csv --y revenue --x revenue_pre --out-md report.md --out-json result.json
```

## Ratio metric (linearized):
```bash
tecore validate --input data.csv --schema b2c_ratio
tecore cuped-ratio --input data.csv --num revenue --den sessions --num-pre revenue_pre --den-pre sessions_pre --out-md report_ratio.md --out-json result_ratio.json
```

## What to send back (pilot feedback)

To evaluate the framework without sharing proprietary data, please send back:

1) Your exact command line used to run the tool (copy/paste).
2) The generated JSON output (`--out-json ...`).
3) The generated Markdown report (`--out-md ...`).

Optional (helps interpretation, still privacy-friendly):
- short description of the metric (what `revenue`/`sessions` represent)
- experiment duration (days) and unit of analysis (user/session/account)
- approximate sample sizes (n_control, n_test)
- whether the pre-period covariate is guaranteed to be measured before exposure (yes/no)
- any notable data properties: heavy tails, many zeros, strong outliers, bot traffic, etc.

If anything fails, please also include:
- the full error text
- your Python version (if known) and OS

## Status

This is an early-stage project. The initial goal is to provide a clear, working baseline for:
- power analysis in B2C experiments,
- basic metric design helpers,
- reproducible examples and benchmarks (synthetic data + notebooks).

Future plans include:
- more advanced designs (e.g. uneven allocation, stratification),
- sequential testing examples,
- additional robust metric patterns and practical templates.

Contributions, issues and suggestions are very welcome.
