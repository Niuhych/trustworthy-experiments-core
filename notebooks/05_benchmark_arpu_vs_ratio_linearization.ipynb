{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2B - Benchmark: ARPU vs ratio metric + linearization\n",
        "\n",
        "This notebook compares three approaches on the same user-level aggregates:\n",
        "\n",
        "- ARPU (mean revenue per user)\n",
        "- Ratio metric: revenue per session (group-level ratio)\n",
        "- Linearization for ratio metrics (user-level transformation + t-test)\n",
        "\n",
        "We evaluate:\n",
        "- effect estimates\n",
        "- confidence intervals\n",
        "- p-values\n",
        "- empirical power / type I error via repeated simulations\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "repo_root = Path('..').resolve()\n",
        "src_path = (repo_root / 'src').resolve()\n",
        "if str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "from tecore.simulate import SyntheticB2CConfig, generate_user_level_data, make_default_scenarios\n",
        "\n",
        "ALPHA = 0.05\n",
        "RNG = np.random.default_rng(123)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load synthetic scenarios (or regenerate)\n",
        "\n",
        "If you already ran notebook 04, we load CSV files.\n",
        "If not, we regenerate them on the fly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "data_dir = repo_root / 'data' / 'synthetic'\n",
        "scenario_files = {\n",
        "    'scenario_monetization': data_dir / 'scenario_monetization.csv',\n",
        "    'scenario_activity': data_dir / 'scenario_activity.csv',\n",
        "    'scenario_mixed': data_dir / 'scenario_mixed.csv',\n",
        "}\n",
        "\n",
        "frames = {}\n",
        "all_exist = all(p.exists() for p in scenario_files.values())\n",
        "\n",
        "if all_exist:\n",
        "    for name, p in scenario_files.items():\n",
        "        frames[name] = pd.read_csv(p)\n",
        "else:\n",
        "    scenarios = make_default_scenarios(SyntheticB2CConfig(n_users=80_000, seed=42))\n",
        "    for name, cfg in scenarios.items():\n",
        "        frames[name] = generate_user_level_data(cfg)\n",
        "\n",
        "list(frames.keys()), all_exist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Helper functions\n",
        "\n",
        "We implement three tests:\n",
        "\n",
        "- ARPU: two-sample t-test on revenue per user\n",
        "- Ratio: group-level ratio difference + bootstrap CI/p-value\n",
        "- Linearization: user-level z = revenue - R0 * sessions and t-test on z\n",
        "\n",
        "Notes:\n",
        "- Linearization uses R0 computed on the control group.\n",
        "- For reporting, we convert linearized mean difference into ratio units.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def split_groups(df: pd.DataFrame):\n",
        "    c = df[df['group'] == 'control'].copy()\n",
        "    t = df[df['group'] == 'test'].copy()\n",
        "    return c, t\n",
        "\n",
        "\n",
        "def arpu_test(df: pd.DataFrame, alpha: float = ALPHA):\n",
        "    c, t = split_groups(df)\n",
        "    y_c, y_t = c['revenue'].to_numpy(), t['revenue'].to_numpy()\n",
        "\n",
        "    # Welch t-test (robust to unequal variances)\n",
        "    stat, pval = stats.ttest_ind(y_t, y_c, equal_var=False)\n",
        "\n",
        "    eff = float(y_t.mean() - y_c.mean())\n",
        "    se = float(np.sqrt(y_t.var(ddof=1)/len(y_t) + y_c.var(ddof=1)/len(y_c)))\n",
        "    z = stats.norm.ppf(1 - alpha/2)\n",
        "    ci = (eff - z*se, eff + z*se)\n",
        "\n",
        "    return {\n",
        "        'metric': 'ARPU',\n",
        "        'effect': eff,\n",
        "        'ci_low': ci[0],\n",
        "        'ci_high': ci[1],\n",
        "        'p_value': float(pval),\n",
        "    }\n",
        "\n",
        "\n",
        "def ratio_point(df: pd.DataFrame) -> float:\n",
        "    return float(df['revenue'].sum() / df['sessions'].sum())\n",
        "\n",
        "\n",
        "def ratio_bootstrap_test(df: pd.DataFrame, n_boot: int = 2000, alpha: float = ALPHA, seed: int = 0):\n",
        "    c, t = split_groups(df)\n",
        "\n",
        "    r_c = ratio_point(c)\n",
        "    r_t = ratio_point(t)\n",
        "    eff = r_t - r_c\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "    c_idx = np.arange(len(c))\n",
        "    t_idx = np.arange(len(t))\n",
        "\n",
        "    boot = np.empty(n_boot, dtype=float)\n",
        "    for i in range(n_boot):\n",
        "        bc = c.iloc[rng.choice(c_idx, size=len(c), replace=True)]\n",
        "        bt = t.iloc[rng.choice(t_idx, size=len(t), replace=True)]\n",
        "        boot[i] = ratio_point(bt) - ratio_point(bc)\n",
        "\n",
        "    lo = float(np.quantile(boot, alpha/2))\n",
        "    hi = float(np.quantile(boot, 1 - alpha/2))\n",
        "\n",
        "    # Two-sided p-value by bootstrap sign test around 0\n",
        "    pval = float(2 * min((boot <= 0).mean(), (boot >= 0).mean()))\n",
        "\n",
        "    return {\n",
        "        'metric': 'Ratio bootstrap',\n",
        "        'effect': float(eff),\n",
        "        'ci_low': lo,\n",
        "        'ci_high': hi,\n",
        "        'p_value': pval,\n",
        "    }\n",
        "\n",
        "\n",
        "def linearization_test(df: pd.DataFrame, alpha: float = ALPHA):\n",
        "    c, t = split_groups(df)\n",
        "\n",
        "    # Baseline ratio from control\n",
        "    R0 = ratio_point(c)\n",
        "\n",
        "    z_c = c['revenue'].to_numpy() - R0 * c['sessions'].to_numpy()\n",
        "    z_t = t['revenue'].to_numpy() - R0 * t['sessions'].to_numpy()\n",
        "\n",
        "    stat, pval = stats.ttest_ind(z_t, z_c, equal_var=False)\n",
        "\n",
        "    # Convert mean difference in z to ratio units\n",
        "    mean_sessions_c = float(c['sessions'].mean())\n",
        "    dz = float(z_t.mean() - z_c.mean())\n",
        "    eff = dz / mean_sessions_c\n",
        "\n",
        "    # CI from normal approximation on dz\n",
        "    se_dz = float(np.sqrt(z_t.var(ddof=1)/len(z_t) + z_c.var(ddof=1)/len(z_c)))\n",
        "    zcrit = stats.norm.ppf(1 - alpha/2)\n",
        "    ci = ((dz - zcrit*se_dz)/mean_sessions_c, (dz + zcrit*se_dz)/mean_sessions_c)\n",
        "\n",
        "    return {\n",
        "        'metric': 'Ratio linearization',\n",
        "        'effect': eff,\n",
        "        'ci_low': float(ci[0]),\n",
        "        'ci_high': float(ci[1]),\n",
        "        'p_value': float(pval),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) One-run comparison for each scenario\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def run_all_methods(df: pd.DataFrame, seed: int = 0) -> pd.DataFrame:\n",
        "    rows = []\n",
        "    rows.append(arpu_test(df))\n",
        "    rows.append(ratio_bootstrap_test(df, n_boot=1200, seed=seed))\n",
        "    rows.append(linearization_test(df))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "for name, df in frames.items():\n",
        "    print('\\n', '='*80)\n",
        "    print(name)\n",
        "    display(run_all_methods(df, seed=123))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Empirical power and type I error via repeated simulations\n",
        "\n",
        "We generate many synthetic datasets and measure how often each method rejects H0 at alpha = 0.05.\n",
        "\n",
        "Two regimes:\n",
        "- effect datasets: effect is present (power)\n",
        "- null datasets: effect is absent (type I error)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def rejection_rate(method_fn, cfg: SyntheticB2CConfig, n_iter: int, alpha: float, seed0: int) -> float:\n",
        "    rej = 0\n",
        "    for i in range(n_iter):\n",
        "        cfg_i = SyntheticB2CConfig(**{**cfg.__dict__, 'seed': seed0 + i})\n",
        "        df = generate_user_level_data(cfg_i)\n",
        "        res = method_fn(df)\n",
        "        if res['p_value'] < alpha:\n",
        "            rej += 1\n",
        "    return rej / n_iter\n",
        "\n",
        "\n",
        "def make_cfg(effect_type: str, lift_sessions: float, lift_revps: float, seed: int, n_users: int = 60_000):\n",
        "    return SyntheticB2CConfig(\n",
        "        n_users=n_users,\n",
        "        seed=seed,\n",
        "        effect_type=effect_type,\n",
        "        lift_sessions=lift_sessions,\n",
        "        lift_rev_per_session=lift_revps,\n",
        "        sessions_mean_pre=3.0,\n",
        "        sessions_mean_post=3.2,\n",
        "        sessions_dispersion=1.6,\n",
        "        high_spender_share=0.03,\n",
        "        revps_lognorm_mu=-0.2,\n",
        "        revps_lognorm_sigma=1.0,\n",
        "        high_revps_multiplier=6.0,\n",
        "        activity_latent_sigma=0.6,\n",
        "        monetization_latent_sigma=0.7,\n",
        "        include_binary=False,\n",
        "    )\n",
        "\n",
        "methods = {\n",
        "    'ARPU': arpu_test,\n",
        "    'Ratio bootstrap': lambda d: ratio_bootstrap_test(d, n_boot=600, seed=1),\n",
        "    'Ratio linearization': linearization_test,\n",
        "}\n",
        "\n",
        "scenarios_cfg = {\n",
        "    'monetization': make_cfg('monetization', lift_sessions=0.0, lift_revps=0.05, seed=10),\n",
        "    'activity': make_cfg('activity', lift_sessions=0.07, lift_revps=0.0, seed=20),\n",
        "    'mixed': make_cfg('mixed', lift_sessions=0.03, lift_revps=0.03, seed=30),\n",
        "}\n",
        "\n",
        "null_cfg = make_cfg('none', lift_sessions=0.0, lift_revps=0.0, seed=999)\n",
        "\n",
        "N_ITER = 120\n",
        "\n",
        "rows = []\n",
        "for scen_name, cfg in scenarios_cfg.items():\n",
        "    for mname, mfn in methods.items():\n",
        "        power = rejection_rate(mfn, cfg, n_iter=N_ITER, alpha=ALPHA, seed0=1000)\n",
        "        type1 = rejection_rate(mfn, null_cfg, n_iter=N_ITER, alpha=ALPHA, seed0=2000)\n",
        "        rows.append({'scenario': scen_name, 'method': mname, 'power': power, 'type1_error': type1})\n",
        "\n",
        "summary = pd.DataFrame(rows)\n",
        "summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Visualize benchmark results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def plot_bars(df: pd.DataFrame, value_col: str, title: str):\n",
        "    scenarios = df['scenario'].unique().tolist()\n",
        "    methods = df['method'].unique().tolist()\n",
        "\n",
        "    x = np.arange(len(scenarios))\n",
        "    width = 0.25\n",
        "\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    for i, m in enumerate(methods):\n",
        "        vals = [float(df[(df['scenario'] == s) & (df['method'] == m)][value_col].iloc[0]) for s in scenarios]\n",
        "        plt.bar(x + (i - 1) * width, vals, width=width, label=m)\n",
        "\n",
        "    plt.xticks(x, scenarios)\n",
        "    plt.ylim(0, 1)\n",
        "    plt.ylabel(value_col)\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_bars(summary, 'power', 'Empirical power by scenario (alpha=0.05)')\n",
        "plot_bars(summary, 'type1_error', 'Empirical type I error under null (alpha=0.05)')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Results summary\n",
        "\n",
        "- Different effect mechanisms favor different primary metrics.\n",
        "- Ratio metric with linearization is often more aligned with monetization-only effects.\n",
        "- Bootstrap ratio test is robust but more computationally expensive.\n",
        "- Under the null, type I error should be close to alpha.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}