{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2D - CUPED with heavy tails and outliers (ARPU)\n",
        "\n",
        "This notebook focuses on a very common real-world issue: **ARPU / revenue is heavy-tailed**.\n",
        "\n",
        "When revenue has outliers, simple t-tests on raw ARPU may behave poorly:\n",
        "- variance becomes huge,\n",
        "- confidence intervals are wide,\n",
        "- empirical power is low at realistic sample sizes.\n",
        "\n",
        "We compare several practical approaches and show how they interact with CUPED:\n",
        "\n",
        "- **Raw ARPU**\n",
        "- **Winsorized ARPU** (cap extreme values)\n",
        "- **Log-ARPU** (`log1p(revenue)` then interpret in log-units)\n",
        "- **Trimmed mean ARPU** (remove a small fraction of extremes)\n",
        "\n",
        "For each approach, we compare:\n",
        "- base (no CUPED)\n",
        "- CUPED using pre-period revenue as a covariate\n",
        "\n",
        "Outputs:\n",
        "- estimated CUPED theta and variance reduction\n",
        "- CI widths and p-values\n",
        "- empirical power / type I error in a small simulation loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0) Imports and setup\n",
        "\n",
        "Expected repo layout:\n",
        "- notebooks/\n",
        "- src/tecore/\n",
        "- data/synthetic/\n",
        "\n",
        "If you run in DataLore without the repository mounted, create local modules:\n",
        "- `tecore/simulate.py`\n",
        "- `tecore/variance_reduction.py`\n",
        "(as you already do with `%%writefile`).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "ALPHA = 0.05\n",
        "RNG = np.random.default_rng(123)\n",
        "\n",
        "# If running from repo notebooks/, this makes `src/` importable:\n",
        "repo_root = Path(\"..\").resolve()\n",
        "src_path = (repo_root / \"src\").resolve()\n",
        "if (src_path / \"tecore\").exists() and str(src_path) not in sys.path:\n",
        "    sys.path.insert(0, str(src_path))\n",
        "\n",
        "from tecore.simulate import SyntheticB2CConfig, generate_user_level_data\n",
        "from tecore.variance_reduction import cuped_split_adjust\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Generate a heavy-tailed dataset\n",
        "\n",
        "We intentionally increase tail heaviness by:\n",
        "- increasing `revps_lognorm_sigma`\n",
        "- increasing `high_spender_share`\n",
        "- increasing `high_revps_multiplier`\n",
        "\n",
        "We keep the dataset user-level and aggregated over the post period.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "cfg = SyntheticB2CConfig(\n",
        "    n_users=80_000,\n",
        "    seed=42,\n",
        "    effect_type=\"mixed\",\n",
        "    lift_sessions=0.03,\n",
        "    lift_rev_per_session=0.03,\n",
        "    include_binary=False,\n",
        "\n",
        "    # Make tails heavier:\n",
        "    revps_lognorm_sigma=1.35,\n",
        "    high_spender_share=0.05,\n",
        "    high_revps_multiplier=10.0,\n",
        "\n",
        "    # Keep decent pre/post correlation for CUPED to work:\n",
        "    monetization_latent_sigma=0.8,\n",
        ")\n",
        "\n",
        "df = generate_user_level_data(cfg)\n",
        "df.shape, df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Quick distribution check\n",
        "\n",
        "We look at:\n",
        "- histogram of revenue (clipped for visibility)\n",
        "- log-histogram\n",
        "- a simple boxplot (showing how extreme the tail is)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "rev = df[\"revenue\"].to_numpy(dtype=float)\n",
        "\n",
        "p99 = np.quantile(rev, 0.99)\n",
        "p999 = np.quantile(rev, 0.999)\n",
        "\n",
        "fig = plt.figure(figsize=(10, 4))\n",
        "plt.hist(np.clip(rev, 0, p99), bins=60)\n",
        "plt.title(\"Revenue per user (clipped at p99 for visibility)\")\n",
        "plt.xlabel(\"revenue\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.hist(np.log1p(rev), bins=60)\n",
        "plt.title(\"log1p(revenue) distribution\")\n",
        "plt.xlabel(\"log1p(revenue)\")\n",
        "plt.ylabel(\"count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 3))\n",
        "plt.boxplot(np.clip(rev, 0, p999), vert=False, showfliers=False)\n",
        "plt.title(\"Revenue per user (clipped at p99.9)\")\n",
        "plt.xlabel(\"revenue\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "p99, p999"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Utility functions\n",
        "\n",
        "We implement:\n",
        "- Welch t-test effect + CI\n",
        "- winsorization and trimming helpers\n",
        "- a unified runner: base vs CUPED for a chosen transformation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def split_groups(df: pd.DataFrame):\n",
        "    c = df[df[\"group\"] == \"control\"].copy()\n",
        "    t = df[df[\"group\"] == \"test\"].copy()\n",
        "    return c, t\n",
        "\n",
        "\n",
        "def welch_ttest_effect(y_c: np.ndarray, y_t: np.ndarray, alpha: float = ALPHA):\n",
        "    stat, pval = stats.ttest_ind(y_t, y_c, equal_var=False)\n",
        "\n",
        "    eff = float(np.mean(y_t) - np.mean(y_c))\n",
        "    se = float(np.sqrt(np.var(y_t, ddof=1)/len(y_t) + np.var(y_c, ddof=1)/len(y_c)))\n",
        "    z = stats.norm.ppf(1 - alpha/2)\n",
        "    ci = (eff - z*se, eff + z*se)\n",
        "\n",
        "    return {\"effect\": eff, \"ci_low\": ci[0], \"ci_high\": ci[1], \"p_value\": float(pval), \"se\": se}\n",
        "\n",
        "\n",
        "def winsorize(x: np.ndarray, lower_q: float = 0.0, upper_q: float = 0.99) -> np.ndarray:\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    lo = np.quantile(x, lower_q)\n",
        "    hi = np.quantile(x, upper_q)\n",
        "    return np.clip(x, lo, hi)\n",
        "\n",
        "\n",
        "def trim_mask(x: np.ndarray, lower_q: float = 0.0, upper_q: float = 0.99) -> np.ndarray:\n",
        "    x = np.asarray(x, dtype=float)\n",
        "    lo = np.quantile(x, lower_q)\n",
        "    hi = np.quantile(x, upper_q)\n",
        "    return (x >= lo) & (x <= hi)\n",
        "\n",
        "\n",
        "def run_base_vs_cuped(\n",
        "    y_c: np.ndarray,\n",
        "    y_t: np.ndarray,\n",
        "    x_c: np.ndarray,\n",
        "    x_t: np.ndarray,\n",
        "    alpha: float = ALPHA,\n",
        "):\n",
        "    base = welch_ttest_effect(y_c, y_t, alpha=alpha)\n",
        "    cup_c, cup_t = cuped_split_adjust(y_c, x_c, y_t, x_t)\n",
        "    adj = welch_ttest_effect(cup_c.y_adj, cup_t.y_adj, alpha=alpha)\n",
        "\n",
        "    return {\n",
        "        \"base\": base,\n",
        "        \"cuped\": {**adj, \"theta\": cup_c.theta, \"var_reduction_control\": cup_c.var_reduction},\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Compare approaches for ARPU\n",
        "\n",
        "Covariate for CUPED:\n",
        "- pre-period revenue (`revenue_pre`)\n",
        "\n",
        "Approaches:\n",
        "1) raw revenue (ARPU)\n",
        "2) winsorized revenue (cap at p99)\n",
        "3) log1p(revenue)\n",
        "4) trimmed mean (drop top 1%)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "c, t = split_groups(df)\n",
        "\n",
        "y_c_raw = c[\"revenue\"].to_numpy(dtype=float)\n",
        "y_t_raw = t[\"revenue\"].to_numpy(dtype=float)\n",
        "x_c_raw = c[\"revenue_pre\"].to_numpy(dtype=float)\n",
        "x_t_raw = t[\"revenue_pre\"].to_numpy(dtype=float)\n",
        "\n",
        "# 1) Raw\n",
        "res_raw = run_base_vs_cuped(y_c_raw, y_t_raw, x_c_raw, x_t_raw)\n",
        "\n",
        "# 2) Winsorized at p99 (apply caps independently per full sample for simplicity)\n",
        "y_all_win = winsorize(df[\"revenue\"].to_numpy(dtype=float), upper_q=0.99)\n",
        "x_all_win = winsorize(df[\"revenue_pre\"].to_numpy(dtype=float), upper_q=0.99)\n",
        "\n",
        "y_c_win = y_all_win[df[\"group\"].to_numpy() == \"control\"]\n",
        "y_t_win = y_all_win[df[\"group\"].to_numpy() == \"test\"]\n",
        "x_c_win = x_all_win[df[\"group\"].to_numpy() == \"control\"]\n",
        "x_t_win = x_all_win[df[\"group\"].to_numpy() == \"test\"]\n",
        "res_win = run_base_vs_cuped(y_c_win, y_t_win, x_c_win, x_t_win)\n",
        "\n",
        "# 3) log1p\n",
        "y_c_log = np.log1p(y_c_raw); y_t_log = np.log1p(y_t_raw)\n",
        "x_c_log = np.log1p(x_c_raw); x_t_log = np.log1p(x_t_raw)\n",
        "res_log = run_base_vs_cuped(y_c_log, y_t_log, x_c_log, x_t_log)\n",
        "\n",
        "# 4) Trimmed (drop top 1% based on each period separately; keep only users within caps)\n",
        "mask_y = trim_mask(df[\"revenue\"].to_numpy(dtype=float), upper_q=0.99)\n",
        "mask_x = trim_mask(df[\"revenue_pre\"].to_numpy(dtype=float), upper_q=0.99)\n",
        "mask = mask_y & mask_x\n",
        "\n",
        "df_trim = df.loc[mask].copy()\n",
        "c2, t2 = split_groups(df_trim)\n",
        "\n",
        "res_trim = run_base_vs_cuped(\n",
        "    c2[\"revenue\"].to_numpy(dtype=float),\n",
        "    t2[\"revenue\"].to_numpy(dtype=float),\n",
        "    c2[\"revenue_pre\"].to_numpy(dtype=float),\n",
        "    t2[\"revenue_pre\"].to_numpy(dtype=float),\n",
        ")\n",
        "\n",
        "def to_row(name: str, res: dict):\n",
        "    return [\n",
        "        {\"approach\": name, \"variant\": \"base\", **res[\"base\"]},\n",
        "        {\"approach\": name, \"variant\": \"CUPED\", **res[\"cuped\"]},\n",
        "    ]\n",
        "\n",
        "rows = []\n",
        "rows += to_row(\"raw_ARPU\", res_raw)\n",
        "rows += to_row(\"winsorized_p99\", res_win)\n",
        "rows += to_row(\"log1p_ARPU\", res_log)\n",
        "rows += to_row(\"trimmed_top1pct\", res_trim)\n",
        "\n",
        "out = pd.DataFrame(rows)\n",
        "out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) CI width comparison\n",
        "\n",
        "A quick way to communicate practical impact is CI width reduction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def ci_width(ci_low: float, ci_high: float) -> float:\n",
        "    return float(ci_high - ci_low)\n",
        "\n",
        "wide = []\n",
        "for appr in out[\"approach\"].unique():\n",
        "    base = out[(out[\"approach\"] == appr) & (out[\"variant\"] == \"base\")].iloc[0]\n",
        "    cup = out[(out[\"approach\"] == appr) & (out[\"variant\"] == \"CUPED\")].iloc[0]\n",
        "    w0 = ci_width(base[\"ci_low\"], base[\"ci_high\"])\n",
        "    w1 = ci_width(cup[\"ci_low\"], cup[\"ci_high\"])\n",
        "    wide.append({\"approach\": appr, \"ci_width_base\": w0, \"ci_width_cuped\": w1, \"reduction\": 1 - w1 / w0})\n",
        "\n",
        "ci_tbl = pd.DataFrame(wide).sort_values(\"reduction\", ascending=False)\n",
        "ci_tbl"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "x = np.arange(len(ci_tbl))\n",
        "plt.bar(x - 0.2, ci_tbl[\"ci_width_base\"], width=0.4, label=\"base\")\n",
        "plt.bar(x + 0.2, ci_tbl[\"ci_width_cuped\"], width=0.4, label=\"CUPED\", alpha=0.7)\n",
        "plt.xticks(x, ci_tbl[\"approach\"], rotation=20, ha=\"right\")\n",
        "plt.ylabel(\"CI width (in approach units)\")\n",
        "plt.title(\"CI width: base vs CUPED under heavy tails\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Empirical power and type I error (small simulation)\n",
        "\n",
        "We estimate rejection rates at fixed sample size.\n",
        "\n",
        "To keep runtime reasonable:\n",
        "- start with N_ITER = 25\n",
        "- no bootstrap inside the loop\n",
        "\n",
        "We compare:\n",
        "- raw ARPU (base vs CUPED)\n",
        "- winsorized p99 (base vs CUPED)\n",
        "- log1p ARPU (base vs CUPED)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def pvals_raw_base_cuped(df: pd.DataFrame, alpha: float = ALPHA):\n",
        "    c, t = split_groups(df)\n",
        "\n",
        "    y_c = c[\"revenue\"].to_numpy(float); y_t = t[\"revenue\"].to_numpy(float)\n",
        "    x_c = c[\"revenue_pre\"].to_numpy(float); x_t = t[\"revenue_pre\"].to_numpy(float)\n",
        "\n",
        "    base = welch_ttest_effect(y_c, y_t, alpha=alpha)[\"p_value\"]\n",
        "    cup_c, cup_t = cuped_split_adjust(y_c, x_c, y_t, x_t)\n",
        "    cup = welch_ttest_effect(cup_c.y_adj, cup_t.y_adj, alpha=alpha)[\"p_value\"]\n",
        "\n",
        "    return base, cup\n",
        "\n",
        "\n",
        "def pvals_winsor_base_cuped(df: pd.DataFrame, alpha: float = ALPHA):\n",
        "    y_all = winsorize(df[\"revenue\"].to_numpy(float), upper_q=0.99)\n",
        "    x_all = winsorize(df[\"revenue_pre\"].to_numpy(float), upper_q=0.99)\n",
        "    g = df[\"group\"].to_numpy()\n",
        "\n",
        "    y_c = y_all[g == \"control\"]; y_t = y_all[g == \"test\"]\n",
        "    x_c = x_all[g == \"control\"]; x_t = x_all[g == \"test\"]\n",
        "\n",
        "    base = welch_ttest_effect(y_c, y_t, alpha=alpha)[\"p_value\"]\n",
        "    cup_c, cup_t = cuped_split_adjust(y_c, x_c, y_t, x_t)\n",
        "    cup = welch_ttest_effect(cup_c.y_adj, cup_t.y_adj, alpha=alpha)[\"p_value\"]\n",
        "\n",
        "    return base, cup\n",
        "\n",
        "\n",
        "def pvals_log_base_cuped(df: pd.DataFrame, alpha: float = ALPHA):\n",
        "    c, t = split_groups(df)\n",
        "\n",
        "    y_c = np.log1p(c[\"revenue\"].to_numpy(float)); y_t = np.log1p(t[\"revenue\"].to_numpy(float))\n",
        "    x_c = np.log1p(c[\"revenue_pre\"].to_numpy(float)); x_t = np.log1p(t[\"revenue_pre\"].to_numpy(float))\n",
        "\n",
        "    base = welch_ttest_effect(y_c, y_t, alpha=alpha)[\"p_value\"]\n",
        "    cup_c, cup_t = cuped_split_adjust(y_c, x_c, y_t, x_t)\n",
        "    cup = welch_ttest_effect(cup_c.y_adj, cup_t.y_adj, alpha=alpha)[\"p_value\"]\n",
        "\n",
        "    return base, cup\n",
        "\n",
        "\n",
        "def rejection_rates(cfg: SyntheticB2CConfig, n_iter: int, alpha: float, seed0: int):\n",
        "    rej = {\n",
        "        \"raw_base\": 0, \"raw_cuped\": 0,\n",
        "        \"win_base\": 0, \"win_cuped\": 0,\n",
        "        \"log_base\": 0, \"log_cuped\": 0,\n",
        "    }\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        cfg_i = SyntheticB2CConfig(**{**cfg.__dict__, \"seed\": seed0 + i})\n",
        "        df_i = generate_user_level_data(cfg_i)\n",
        "\n",
        "        p0, p1 = pvals_raw_base_cuped(df_i, alpha=alpha)\n",
        "        rej[\"raw_base\"] += (p0 < alpha); rej[\"raw_cuped\"] += (p1 < alpha)\n",
        "\n",
        "        p0, p1 = pvals_winsor_base_cuped(df_i, alpha=alpha)\n",
        "        rej[\"win_base\"] += (p0 < alpha); rej[\"win_cuped\"] += (p1 < alpha)\n",
        "\n",
        "        p0, p1 = pvals_log_base_cuped(df_i, alpha=alpha)\n",
        "        rej[\"log_base\"] += (p0 < alpha); rej[\"log_cuped\"] += (p1 < alpha)\n",
        "\n",
        "    for k in rej:\n",
        "        rej[k] = rej[k] / n_iter\n",
        "\n",
        "    return rej\n",
        "\n",
        "\n",
        "N_ITER = 25\n",
        "\n",
        "cfg_eff = SyntheticB2CConfig(\n",
        "    n_users=60_000,\n",
        "    seed=10,\n",
        "    effect_type=\"mixed\",\n",
        "    lift_sessions=0.02,\n",
        "    lift_rev_per_session=0.02,\n",
        "    include_binary=False,\n",
        "    revps_lognorm_sigma=1.35,\n",
        "    high_spender_share=0.05,\n",
        "    high_revps_multiplier=10.0,\n",
        "    monetization_latent_sigma=0.8,\n",
        ")\n",
        "\n",
        "cfg_null = SyntheticB2CConfig(\n",
        "    n_users=60_000,\n",
        "    seed=999,\n",
        "    effect_type=\"none\",\n",
        "    lift_sessions=0.0,\n",
        "    lift_rev_per_session=0.0,\n",
        "    include_binary=False,\n",
        "    revps_lognorm_sigma=1.35,\n",
        "    high_spender_share=0.05,\n",
        "    high_revps_multiplier=10.0,\n",
        "    monetization_latent_sigma=0.8,\n",
        ")\n",
        "\n",
        "power = rejection_rates(cfg_eff, n_iter=N_ITER, alpha=ALPHA, seed0=5000)\n",
        "type1 = rejection_rates(cfg_null, n_iter=N_ITER, alpha=ALPHA, seed0=6000)\n",
        "\n",
        "pd.DataFrame([\n",
        "    {\"case\": \"power (effect)\", **power},\n",
        "    {\"case\": \"type I error (null)\", **type1},\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes on interpretation\n",
        "\n",
        "- **Raw ARPU** may have low power under heavy tails because variance explodes.\n",
        "- **Winsorization** often improves stability at the cost of introducing a small bias in the estimand\n",
        "  (you are no longer estimating the true mean, but a capped mean).\n",
        "- **log1p transform** changes the estimand. You test differences in expected log-revenue, which is closer\n",
        "  to relative changes and is often more robust.\n",
        "- **CUPED** can stack with winsorization/log transforms: you reduce variance via both\n",
        "  (a) a better-behaved metric and (b) correlated pre-period information.\n",
        "\n",
        "In production experiments, choose the approach based on:\n",
        "- your official KPI definition (do you need the mean, or a robust proxy?)\n",
        "- sensitivity vs interpretability trade-offs\n",
        "- stability of pre-period covariate and absence of treatment leakage into pre-period data\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}